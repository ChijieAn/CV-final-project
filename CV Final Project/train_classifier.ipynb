{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zob_6G8keYVc",
        "outputId": "99589977-4c72-4629-9cde-6af9e8db9ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
        "trainset=torchvision.datasets.CIFAR10(root='./data',train=True,download=True, transform=transform)\n",
        "trainloader=torch.utils.data.DataLoader(trainset,batch_size=4,shuffle=True,num_workers=2)\n",
        "testset=torchvision.datasets.CIFAR10(root='./data',train=False,download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkaF_LPZemZU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "from abc import abstractmethod\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewR43VoXfnPD",
        "outputId": "5bc55fca-0ca0-47cf-fbeb-f737338eafc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw24qO-7hwG4"
      },
      "outputs": [],
      "source": [
        "# beta schedule\n",
        "def linear_beta_schedule(timesteps):\n",
        "    scale = 1000 / timesteps\n",
        "    beta_start = scale * 0.0001\n",
        "    beta_end = scale * 0.02\n",
        "    return torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float64)\n",
        "\n",
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    \"\"\"\n",
        "    cosine schedule\n",
        "    as proposed in https://arxiv.org/abs/2102.09672\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps, dtype=torch.float64)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0, 0.999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Xib0fwgepHc"
      },
      "outputs": [],
      "source": [
        "class GaussianDiffusion:\n",
        "    def __init__(\n",
        "        self,\n",
        "        timesteps=1000,\n",
        "        beta_schedule='linear',\n",
        "        #the label of the image we want to generate\n",
        "        #label\n",
        "        ):\n",
        "        self.timesteps = timesteps\n",
        "\n",
        "        if beta_schedule == 'linear':\n",
        "            betas = linear_beta_schedule(timesteps)\n",
        "        elif beta_schedule == 'cosine':\n",
        "            betas = cosine_beta_schedule(timesteps)\n",
        "        else:\n",
        "            raise ValueError(f'unknown beta schedule {beta_schedule}')\n",
        "        self.betas = betas\n",
        "\n",
        "        self.alphas = 1. - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
        "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.)\n",
        "\n",
        "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
        "        self.log_one_minus_alphas_cumprod = torch.log(1.0 - self.alphas_cumprod)\n",
        "        self.sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod)\n",
        "        self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod - 1)\n",
        "\n",
        "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
        "        self.posterior_variance = (\n",
        "            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
        "        )\n",
        "        # below: log calculation clipped because the posterior variance is 0 at the beginning\n",
        "        # of the diffusion chain\n",
        "        self.posterior_log_variance_clipped = torch.log(self.posterior_variance.clamp(min =1e-20))\n",
        "\n",
        "        self.posterior_mean_coef1 = (\n",
        "            self.betas * torch.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
        "        )\n",
        "        self.posterior_mean_coef2 = (\n",
        "            (1.0 - self.alphas_cumprod_prev)\n",
        "            * torch.sqrt(self.alphas)\n",
        "            / (1.0 - self.alphas_cumprod)\n",
        "        )\n",
        "\n",
        "        #suppose we want to generate the image with label y\n",
        "        #self.label=label\n",
        "\n",
        "\n",
        "\n",
        "    # get the param of given timestep t\n",
        "    def _extract(self, a, t, x_shape):\n",
        "        batch_size = t.shape[0]\n",
        "        out = a.to(t.device).gather(0, t).float()\n",
        "        out = out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
        "        return out\n",
        "\n",
        "    # forward diffusion (using the nice property): q(x_t | x_0)\n",
        "    def q_sample(self, x_start, t, noise=None):\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_start)\n",
        "\n",
        "        sqrt_alphas_cumprod_t = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n",
        "        sqrt_one_minus_alphas_cumprod_t = self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
        "\n",
        "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
        "\n",
        "    # Get the mean and variance of q(x_t | x_0).\n",
        "    def q_mean_variance(self, x_start, t):\n",
        "        mean = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
        "        variance = self._extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
        "        log_variance = self._extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n",
        "        return mean, variance, log_variance\n",
        "\n",
        "    # Compute the mean and variance of the diffusion posterior: q(x_{t-1} | x_t, x_0)\n",
        "    def q_posterior_mean_variance(self, x_start, x_t, t):\n",
        "        posterior_mean = (\n",
        "            self._extract(self.posterior_mean_coef1, t, x_t.shape) * x_start\n",
        "            + self._extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
        "        )\n",
        "        posterior_variance = self._extract(self.posterior_variance, t, x_t.shape)\n",
        "        posterior_log_variance_clipped = self._extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
        "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
        "\n",
        "    # compute x_0 from x_t and pred noise: the reverse of `q_sample`\n",
        "    def predict_start_from_noise(self, x_t, t, noise):\n",
        "        return (\n",
        "            self._extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
        "            self._extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
        "        )\n",
        "\n",
        "\n",
        "    #get the gradient of the classifier on the input x\n",
        "    def get_gradient(self,x_t,x,y):\n",
        "        with torch.enable_grad():\n",
        "          x_t_in=x.detach().requires_grad_(True)\n",
        "          label_tensor = classifier(x_t)\n",
        "          selected=label_tensor[y]\n",
        "\n",
        "          return torch.autograd.grad(y,x_t_in)[0]*arg.classifier_scale\n",
        "\n",
        "\n",
        "    # compute predicted mean and variance of p(x_{t-1} | x_t)\n",
        "    def p_mean_variance(self, model, x_t, t, clip_denoised=True):\n",
        "        # predict noise using model\n",
        "        pred_noise = model(x_t, t)\n",
        "        # get the predicted x_0: different from the algorithm2 in the paper\n",
        "        x_recon = self.predict_start_from_noise(x_t, t, pred_noise)\n",
        "        if clip_denoised:\n",
        "            x_recon = torch.clamp(x_recon, min=-1., max=1.)\n",
        "        model_mean, posterior_variance, posterior_log_variance = \\\n",
        "                    self.q_posterior_mean_variance(x_recon, x_t, t)\n",
        "\n",
        "\n",
        "        #we add the gradient to the model mean to direct the model to generate the image to the direction we want\n",
        "\n",
        "        gradient=self.get_gradient(x_t,x,y)\n",
        "\n",
        "        model_mean+=model+posterior_variance*gradient\n",
        "\n",
        "        return model_mean, posterior_variance, posterior_log_variance\n",
        "\n",
        "    # denoise_step: sample x_{t-1} from x_t and pred_noise\n",
        "    @torch.no_grad()\n",
        "    def p_sample(self, model, x_t, t, clip_denoised=True):\n",
        "        # predict mean and variance\n",
        "        model_mean, _, model_log_variance = self.p_mean_variance(model, x_t, t,\n",
        "                                                    clip_denoised=clip_denoised)\n",
        "        noise = torch.randn_like(x_t)\n",
        "        # no noise when t == 0\n",
        "        nonzero_mask = ((t != 0).float().view(-1, *([1] * (len(x_t.shape) - 1))))\n",
        "        # compute x_{t-1}\n",
        "        pred_img = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
        "        return pred_img\n",
        "\n",
        "    # denoise: reverse diffusion\n",
        "    @torch.no_grad()\n",
        "    def p_sample_loop(self, model, shape):\n",
        "        batch_size = shape[0]\n",
        "        device = next(model.parameters()).device\n",
        "        # start from pure noise (for each example in the batch)\n",
        "        img = torch.randn(shape, device=device)\n",
        "        imgs = []\n",
        "        for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
        "            img = self.p_sample(model, img, torch.full((batch_size,), i, device=device, dtype=torch.long))\n",
        "            imgs.append(img.cpu().numpy())\n",
        "        return imgs\n",
        "\n",
        "    # sample new images\n",
        "    @torch.no_grad()\n",
        "    def sample(self, model, image_size, batch_size=8, channels=3):\n",
        "        return self.p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))\n",
        "\n",
        "    # compute train losses\n",
        "    def train_losses(self, model, x_start, t):\n",
        "        # generate random noise\n",
        "        noise = torch.randn_like(x_start)\n",
        "        # get x_t\n",
        "        x_noisy = self.q_sample(x_start, t, noise=noise)\n",
        "        predicted_noise = model(x_noisy, t)\n",
        "        loss = F.mse_loss(noise, predicted_noise)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE5wrXWtftHo",
        "outputId": "e7756ee7-8df4-49e2-bf86-a5b0a9f76ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Download and prepare CIFAR-10 dataset\n",
        "image_size = 128\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.PILToTensor(),\n",
        "    transforms.ConvertImageDtype(torch.float),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "cifar10_data = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "data_loader = torch.utils.data.DataLoader(cifar10_data, batch_size=128, shuffle=True)\n",
        "\n",
        "gaussian_diffusion = GaussianDiffusion(timesteps=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "798JgJADmDjJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "train_dataset_full, test_dataset_full = train_test_split(cifar10_data, test_size=0.2, random_state=42)\n",
        "#train_dataset_full = CustomTensorDataset(train_df_full)\n",
        "#test_dataset_full = CustomTensorDataset(test_df_full)\n",
        "train_loader_full = DataLoader(train_dataset_full, batch_size=32, shuffle=True)\n",
        "test_loader_full = DataLoader(test_dataset_full, batch_size=32, shuffle=False)\n",
        "train_loader_full_list=[train_loader_full]\n",
        "test_loader_full_list=[test_loader_full]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HRXnRSajQ-S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "#create new dataloaders which contain 1000 images which are randomly chosen from cifar-10\n",
        "full_dataset = data_loader.dataset\n",
        "\n",
        "def get_subset_loader_1000(full_dataset,datasize):\n",
        "\n",
        "  # 随机抽取1000个不重复的索引\n",
        "  subset_indices = torch.randperm(len(full_dataset))[:datasize]\n",
        "\n",
        "  # 创建一个数据子集\n",
        "  subset = Subset(full_dataset, subset_indices)\n",
        "\n",
        "  # 创建一个新的DataLoader，它只从子集中加载数据\n",
        "  # 注意: 你可能想保持与原始DataLoader相同的batch_size和其他参数\n",
        "  subset_dataloader = DataLoader(subset, batch_size=data_loader.batch_size, shuffle=True)\n",
        "\n",
        "  return subset,subset_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIPRErFLRxDD"
      },
      "outputs": [],
      "source": [
        "def get_clean_data(sub_data_loader):\n",
        "    print('Function called.')\n",
        "    results = []\n",
        "    print('Initial results length:', len(results))\n",
        "\n",
        "    for images, labels in sub_data_loader:\n",
        "        for i in range(len(images)):\n",
        "           #print(len(images))\n",
        "            x_start = images[i]\n",
        "\n",
        "            result = {\n",
        "                'noisy': x_start,\n",
        "                'label': labels[i].item()\n",
        "            }\n",
        "            results.append(result)\n",
        "        print('Batch done. Current results length:', len(results))\n",
        "\n",
        "    print('Final results length:', len(results))\n",
        "    df = pd.DataFrame(results)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5gMIU8BQbNB",
        "outputId": "4fcc75af-716f-42c4-f14f-c3c4592ad22b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function called.\n",
            "Initial results length: 0\n",
            "Batch done. Current results length: 128\n",
            "Batch done. Current results length: 256\n",
            "Batch done. Current results length: 384\n",
            "Batch done. Current results length: 512\n",
            "Batch done. Current results length: 640\n",
            "Batch done. Current results length: 768\n",
            "Batch done. Current results length: 896\n",
            "Batch done. Current results length: 1024\n",
            "Batch done. Current results length: 1152\n",
            "Batch done. Current results length: 1280\n",
            "Batch done. Current results length: 1408\n",
            "Batch done. Current results length: 1536\n",
            "Batch done. Current results length: 1664\n",
            "Batch done. Current results length: 1792\n",
            "Batch done. Current results length: 1920\n",
            "Batch done. Current results length: 2048\n",
            "Batch done. Current results length: 2176\n",
            "Batch done. Current results length: 2304\n",
            "Batch done. Current results length: 2432\n",
            "Batch done. Current results length: 2560\n",
            "Batch done. Current results length: 2688\n",
            "Batch done. Current results length: 2816\n",
            "Batch done. Current results length: 2944\n",
            "Batch done. Current results length: 3072\n",
            "Batch done. Current results length: 3200\n",
            "Batch done. Current results length: 3328\n",
            "Batch done. Current results length: 3456\n",
            "Batch done. Current results length: 3584\n",
            "Batch done. Current results length: 3712\n",
            "Batch done. Current results length: 3840\n",
            "Batch done. Current results length: 3968\n",
            "Batch done. Current results length: 4096\n",
            "Batch done. Current results length: 4224\n",
            "Batch done. Current results length: 4352\n",
            "Batch done. Current results length: 4480\n",
            "Batch done. Current results length: 4608\n",
            "Batch done. Current results length: 4736\n",
            "Batch done. Current results length: 4864\n",
            "Batch done. Current results length: 4992\n",
            "Batch done. Current results length: 5120\n",
            "Batch done. Current results length: 5248\n",
            "Batch done. Current results length: 5376\n",
            "Batch done. Current results length: 5504\n",
            "Batch done. Current results length: 5632\n",
            "Batch done. Current results length: 5760\n",
            "Batch done. Current results length: 5888\n",
            "Batch done. Current results length: 6016\n",
            "Batch done. Current results length: 6144\n",
            "Batch done. Current results length: 6272\n",
            "Batch done. Current results length: 6400\n",
            "Batch done. Current results length: 6528\n",
            "Batch done. Current results length: 6656\n",
            "Batch done. Current results length: 6784\n",
            "Batch done. Current results length: 6912\n",
            "Batch done. Current results length: 7040\n",
            "Batch done. Current results length: 7168\n",
            "Batch done. Current results length: 7296\n",
            "Batch done. Current results length: 7424\n",
            "Batch done. Current results length: 7552\n",
            "Batch done. Current results length: 7680\n",
            "Batch done. Current results length: 7808\n",
            "Batch done. Current results length: 7936\n",
            "Batch done. Current results length: 8064\n",
            "Batch done. Current results length: 8192\n",
            "Batch done. Current results length: 8320\n",
            "Batch done. Current results length: 8448\n",
            "Batch done. Current results length: 8576\n",
            "Batch done. Current results length: 8704\n",
            "Batch done. Current results length: 8832\n",
            "Batch done. Current results length: 8960\n",
            "Batch done. Current results length: 9088\n",
            "Batch done. Current results length: 9216\n",
            "Batch done. Current results length: 9344\n",
            "Batch done. Current results length: 9472\n",
            "Batch done. Current results length: 9600\n",
            "Batch done. Current results length: 9728\n",
            "Batch done. Current results length: 9856\n",
            "Batch done. Current results length: 9984\n",
            "Batch done. Current results length: 10000\n",
            "Final results length: 10000\n"
          ]
        }
      ],
      "source": [
        "#get clean train test loader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset_10000,dataset_10000_loader=get_subset_loader_1000(cifar10_data,10000)\n",
        "#train_df_clean, test_df_clean = train_test_split(dataset_10000, test_size=0.2, random_state=42)\n",
        "clean_dataframe=get_clean_data(dataset_10000_loader)\n",
        "train_df_clean, test_df_clean = train_test_split(clean_dataframe, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Uve1JEsS6Qw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "c6ff1f89-155b-4019-fa72-915a17f61831"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-825dc81a6a15>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_dataset_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_loader_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_clean\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_loader_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_loader_clean_lst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_loader_clean\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CustomTensorDataset' is not defined"
          ]
        }
      ],
      "source": [
        "train_dataset_clean = CustomTensorDataset(train_df_clean)\n",
        "test_dataset_clean = CustomTensorDataset(test_df_clean)\n",
        "train_loader_clean = DataLoader(train_dataset_clean , batch_size=32, shuffle=True)\n",
        "test_loader_clean = DataLoader(test_dataset_clean, batch_size=32, shuffle=False)\n",
        "train_loader_clean_lst=[train_loader_clean]\n",
        "test_loader_clean_lst=[test_loader_clean]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YmcrgSqjsY5",
        "outputId": "772d48db-ea87-4b15-c2c0-7adc1f554b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataset.Subset object at 0x7fee813e3d90>\n",
            "Size of subset: 5000\n",
            "First data point (features, label): (tensor([[[0.7804, 0.7804, 0.7804,  ..., 0.9294, 0.9294, 0.9294],\n",
            "         [0.7804, 0.7804, 0.7804,  ..., 0.9294, 0.9294, 0.9294],\n",
            "         [0.7804, 0.7804, 0.7804,  ..., 0.9137, 0.9137, 0.9137],\n",
            "         ...,\n",
            "         [0.3725, 0.3725, 0.3725,  ..., 0.1608, 0.1608, 0.1608],\n",
            "         [0.3725, 0.3725, 0.3725,  ..., 0.1686, 0.1686, 0.1686],\n",
            "         [0.3725, 0.3725, 0.3725,  ..., 0.1686, 0.1686, 0.1686]],\n",
            "\n",
            "        [[0.7647, 0.7647, 0.7647,  ..., 0.9294, 0.9294, 0.9294],\n",
            "         [0.7647, 0.7647, 0.7647,  ..., 0.9294, 0.9294, 0.9294],\n",
            "         [0.7647, 0.7647, 0.7647,  ..., 0.9137, 0.9137, 0.9137],\n",
            "         ...,\n",
            "         [0.2941, 0.2941, 0.2941,  ..., 0.0902, 0.0902, 0.0902],\n",
            "         [0.2941, 0.2941, 0.2941,  ..., 0.0980, 0.0980, 0.0980],\n",
            "         [0.2941, 0.2941, 0.2941,  ..., 0.0980, 0.0980, 0.0980]],\n",
            "\n",
            "        [[0.8588, 0.8588, 0.8588,  ..., 0.9294, 0.9294, 0.9294],\n",
            "         [0.8588, 0.8588, 0.8588,  ..., 0.9294, 0.9294, 0.9294],\n",
            "         [0.8588, 0.8588, 0.8588,  ..., 0.9137, 0.9137, 0.9137],\n",
            "         ...,\n",
            "         [0.1922, 0.1922, 0.1922,  ..., 0.0118, 0.0118, 0.0118],\n",
            "         [0.1922, 0.1922, 0.1922,  ..., 0.0196, 0.0196, 0.0196],\n",
            "         [0.1922, 0.1922, 0.1922,  ..., 0.0196, 0.0196, 0.0196]]]), 9)\n"
          ]
        }
      ],
      "source": [
        "subset,_t=get_subset_loader_1000(full_dataset,5000)\n",
        "print(subset)\n",
        "print(f\"Size of subset: {len(subset)}\")\n",
        "first_data_point = subset[0]\n",
        "print(f\"First data point (features, label): {first_data_point}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhgsoUpfiAdn"
      },
      "outputs": [],
      "source": [
        "#generate the date for training the classifier\n",
        "#the ourput of the function contains 1000 data of the noisy image in the corresponding timestep interval. In which we take 1000 images randomly in cifar 10 dataset and add noise to a random step\n",
        "#in the interval we set\n",
        "import random\n",
        "\n",
        "def get_training_data(begin, end, sub_data_loader):\n",
        "    print('Function called.')\n",
        "    results = []\n",
        "    print('Initial results length:', len(results))\n",
        "\n",
        "    for images, labels in sub_data_loader:\n",
        "        for i in range(len(images)):\n",
        "           #print(len(images))\n",
        "            x_start = images[i]\n",
        "            #print('this is x_start',x_start)\n",
        "            t = random.randint(begin, end)\n",
        "            #print('this is timestep',t)\n",
        "            x_noisy = gaussian_diffusion.q_sample(x_start, t=torch.tensor([t]))\n",
        "\n",
        "            result = {\n",
        "                'noisy': x_noisy,\n",
        "                'label': labels[i].item()\n",
        "            }\n",
        "            results.append(result)\n",
        "        print('Batch done. Current results length:', len(results))\n",
        "\n",
        "    print('Final results length:', len(results))\n",
        "    df = pd.DataFrame(results)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HblNvOvsm6pt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "455d7926-8621-4e89-be9d-d3d729a50ddd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function called.\n",
            "Initial results length: 0\n",
            "Batch done. Current results length: 128\n",
            "Batch done. Current results length: 256\n",
            "Batch done. Current results length: 384\n",
            "Batch done. Current results length: 512\n",
            "Batch done. Current results length: 640\n",
            "Batch done. Current results length: 768\n",
            "Batch done. Current results length: 896\n",
            "Batch done. Current results length: 1024\n",
            "Batch done. Current results length: 1152\n",
            "Batch done. Current results length: 1280\n",
            "Batch done. Current results length: 1408\n",
            "Batch done. Current results length: 1536\n",
            "Batch done. Current results length: 1664\n",
            "Batch done. Current results length: 1792\n",
            "Batch done. Current results length: 1920\n",
            "Batch done. Current results length: 2048\n",
            "Batch done. Current results length: 2176\n",
            "Batch done. Current results length: 2304\n",
            "Batch done. Current results length: 2432\n",
            "Batch done. Current results length: 2560\n",
            "Batch done. Current results length: 2688\n",
            "Batch done. Current results length: 2816\n",
            "Batch done. Current results length: 2944\n",
            "Batch done. Current results length: 3072\n",
            "Batch done. Current results length: 3200\n",
            "Batch done. Current results length: 3328\n",
            "Batch done. Current results length: 3456\n",
            "Batch done. Current results length: 3584\n",
            "Batch done. Current results length: 3712\n",
            "Batch done. Current results length: 3840\n",
            "Batch done. Current results length: 3968\n",
            "Batch done. Current results length: 4096\n",
            "Batch done. Current results length: 4224\n",
            "Batch done. Current results length: 4352\n",
            "Batch done. Current results length: 4480\n",
            "Batch done. Current results length: 4608\n",
            "Batch done. Current results length: 4736\n",
            "Batch done. Current results length: 4864\n",
            "Batch done. Current results length: 4992\n",
            "Batch done. Current results length: 5120\n",
            "Batch done. Current results length: 5248\n",
            "Batch done. Current results length: 5376\n",
            "Batch done. Current results length: 5504\n",
            "Batch done. Current results length: 5632\n",
            "Batch done. Current results length: 5760\n",
            "Batch done. Current results length: 5888\n",
            "Batch done. Current results length: 6016\n",
            "Batch done. Current results length: 6144\n",
            "Batch done. Current results length: 6272\n",
            "Batch done. Current results length: 6400\n",
            "Batch done. Current results length: 6528\n",
            "Batch done. Current results length: 6656\n",
            "Batch done. Current results length: 6784\n",
            "Batch done. Current results length: 6912\n",
            "Batch done. Current results length: 7040\n",
            "Batch done. Current results length: 7168\n",
            "Batch done. Current results length: 7296\n",
            "Batch done. Current results length: 7424\n",
            "Batch done. Current results length: 7552\n",
            "Batch done. Current results length: 7680\n",
            "Batch done. Current results length: 7808\n",
            "Batch done. Current results length: 7936\n",
            "Batch done. Current results length: 8064\n",
            "Batch done. Current results length: 8192\n",
            "Batch done. Current results length: 8320\n",
            "Batch done. Current results length: 8448\n",
            "Batch done. Current results length: 8576\n",
            "Batch done. Current results length: 8704\n",
            "Batch done. Current results length: 8832\n",
            "Batch done. Current results length: 8960\n",
            "Batch done. Current results length: 9088\n",
            "Batch done. Current results length: 9216\n",
            "Batch done. Current results length: 9344\n",
            "Batch done. Current results length: 9472\n",
            "Batch done. Current results length: 9600\n",
            "Batch done. Current results length: 9728\n",
            "Batch done. Current results length: 9856\n",
            "Batch done. Current results length: 9984\n",
            "Batch done. Current results length: 10112\n",
            "Batch done. Current results length: 10240\n",
            "Batch done. Current results length: 10368\n",
            "Batch done. Current results length: 10496\n",
            "Batch done. Current results length: 10624\n",
            "Batch done. Current results length: 10752\n",
            "Batch done. Current results length: 10880\n",
            "Batch done. Current results length: 11008\n",
            "Batch done. Current results length: 11136\n",
            "Batch done. Current results length: 11264\n",
            "Batch done. Current results length: 11392\n",
            "Batch done. Current results length: 11520\n",
            "Batch done. Current results length: 11648\n",
            "Batch done. Current results length: 11776\n",
            "Batch done. Current results length: 11904\n",
            "Batch done. Current results length: 12032\n",
            "Batch done. Current results length: 12160\n",
            "Batch done. Current results length: 12288\n",
            "Batch done. Current results length: 12416\n",
            "Batch done. Current results length: 12544\n",
            "Batch done. Current results length: 12672\n",
            "Batch done. Current results length: 12800\n",
            "Batch done. Current results length: 12928\n",
            "Batch done. Current results length: 13056\n",
            "Batch done. Current results length: 13184\n",
            "Batch done. Current results length: 13312\n",
            "Batch done. Current results length: 13440\n",
            "Batch done. Current results length: 13568\n",
            "Batch done. Current results length: 13696\n",
            "Batch done. Current results length: 13824\n",
            "Batch done. Current results length: 13952\n",
            "Batch done. Current results length: 14080\n",
            "Batch done. Current results length: 14208\n",
            "Batch done. Current results length: 14336\n",
            "Batch done. Current results length: 14464\n",
            "Batch done. Current results length: 14592\n",
            "Batch done. Current results length: 14720\n",
            "Batch done. Current results length: 14848\n",
            "Batch done. Current results length: 14976\n",
            "Batch done. Current results length: 15104\n",
            "Batch done. Current results length: 15232\n",
            "Batch done. Current results length: 15360\n",
            "Batch done. Current results length: 15488\n",
            "Batch done. Current results length: 15616\n",
            "Batch done. Current results length: 15744\n",
            "Batch done. Current results length: 15872\n",
            "Batch done. Current results length: 16000\n",
            "Batch done. Current results length: 16128\n",
            "Batch done. Current results length: 16256\n",
            "Batch done. Current results length: 16384\n",
            "Batch done. Current results length: 16512\n",
            "Batch done. Current results length: 16640\n",
            "Batch done. Current results length: 16768\n",
            "Batch done. Current results length: 16896\n",
            "Batch done. Current results length: 17024\n",
            "Batch done. Current results length: 17152\n",
            "Batch done. Current results length: 17280\n",
            "Batch done. Current results length: 17408\n",
            "Batch done. Current results length: 17536\n",
            "Batch done. Current results length: 17664\n",
            "Batch done. Current results length: 17792\n",
            "Batch done. Current results length: 17920\n",
            "Batch done. Current results length: 18048\n",
            "Batch done. Current results length: 18176\n",
            "Batch done. Current results length: 18304\n",
            "Batch done. Current results length: 18432\n",
            "Batch done. Current results length: 18560\n",
            "Batch done. Current results length: 18688\n",
            "Batch done. Current results length: 18816\n",
            "Batch done. Current results length: 18944\n",
            "Batch done. Current results length: 19072\n",
            "Batch done. Current results length: 19200\n",
            "Batch done. Current results length: 19328\n",
            "Batch done. Current results length: 19456\n",
            "Batch done. Current results length: 19584\n",
            "Batch done. Current results length: 19712\n",
            "Batch done. Current results length: 19840\n",
            "Batch done. Current results length: 19968\n",
            "Batch done. Current results length: 20096\n",
            "Batch done. Current results length: 20224\n",
            "Batch done. Current results length: 20352\n",
            "Batch done. Current results length: 20480\n",
            "Batch done. Current results length: 20608\n",
            "Batch done. Current results length: 20736\n",
            "Batch done. Current results length: 20864\n",
            "Batch done. Current results length: 20992\n",
            "Batch done. Current results length: 21120\n",
            "Batch done. Current results length: 21248\n",
            "Batch done. Current results length: 21376\n",
            "Batch done. Current results length: 21504\n",
            "Batch done. Current results length: 21632\n",
            "Batch done. Current results length: 21760\n",
            "Batch done. Current results length: 21888\n",
            "Batch done. Current results length: 22016\n",
            "Batch done. Current results length: 22144\n",
            "Batch done. Current results length: 22272\n",
            "Batch done. Current results length: 22400\n",
            "Batch done. Current results length: 22528\n",
            "Batch done. Current results length: 22656\n",
            "Batch done. Current results length: 22784\n",
            "Batch done. Current results length: 22912\n",
            "Batch done. Current results length: 23040\n",
            "Batch done. Current results length: 23168\n",
            "Batch done. Current results length: 23296\n",
            "Batch done. Current results length: 23424\n",
            "Batch done. Current results length: 23552\n",
            "Batch done. Current results length: 23680\n",
            "Batch done. Current results length: 23808\n",
            "Batch done. Current results length: 23936\n",
            "Batch done. Current results length: 24064\n",
            "Batch done. Current results length: 24192\n",
            "Batch done. Current results length: 24320\n",
            "Batch done. Current results length: 24448\n",
            "Batch done. Current results length: 24576\n",
            "Batch done. Current results length: 24704\n",
            "Batch done. Current results length: 24832\n",
            "Batch done. Current results length: 24960\n",
            "Batch done. Current results length: 25088\n",
            "Batch done. Current results length: 25216\n",
            "Batch done. Current results length: 25344\n",
            "Batch done. Current results length: 25472\n",
            "Batch done. Current results length: 25600\n",
            "Batch done. Current results length: 25728\n",
            "Batch done. Current results length: 25856\n",
            "Batch done. Current results length: 25984\n",
            "Batch done. Current results length: 26112\n",
            "Batch done. Current results length: 26240\n",
            "Batch done. Current results length: 26368\n",
            "Batch done. Current results length: 26496\n",
            "Batch done. Current results length: 26624\n",
            "Batch done. Current results length: 26752\n",
            "Batch done. Current results length: 26880\n",
            "Batch done. Current results length: 27008\n",
            "Batch done. Current results length: 27136\n",
            "Batch done. Current results length: 27264\n",
            "Batch done. Current results length: 27392\n",
            "Batch done. Current results length: 27520\n",
            "Batch done. Current results length: 27648\n",
            "Batch done. Current results length: 27776\n",
            "Batch done. Current results length: 27904\n",
            "Batch done. Current results length: 28032\n",
            "Batch done. Current results length: 28160\n",
            "Batch done. Current results length: 28288\n",
            "Batch done. Current results length: 28416\n",
            "Batch done. Current results length: 28544\n",
            "Batch done. Current results length: 28672\n",
            "Batch done. Current results length: 28800\n",
            "Batch done. Current results length: 28928\n",
            "Batch done. Current results length: 29056\n",
            "Batch done. Current results length: 29184\n",
            "Batch done. Current results length: 29312\n",
            "Batch done. Current results length: 29440\n",
            "Batch done. Current results length: 29568\n",
            "Batch done. Current results length: 29696\n",
            "Batch done. Current results length: 29824\n",
            "Batch done. Current results length: 29952\n",
            "Batch done. Current results length: 30080\n",
            "Batch done. Current results length: 30208\n",
            "Batch done. Current results length: 30336\n",
            "Batch done. Current results length: 30464\n",
            "Batch done. Current results length: 30592\n",
            "Batch done. Current results length: 30720\n",
            "Batch done. Current results length: 30848\n",
            "Batch done. Current results length: 30976\n",
            "Batch done. Current results length: 31104\n",
            "Batch done. Current results length: 31232\n",
            "Batch done. Current results length: 31360\n",
            "Batch done. Current results length: 31488\n",
            "Batch done. Current results length: 31616\n",
            "Batch done. Current results length: 31744\n",
            "Batch done. Current results length: 31872\n",
            "Batch done. Current results length: 32000\n",
            "Batch done. Current results length: 32128\n",
            "Batch done. Current results length: 32256\n",
            "Batch done. Current results length: 32384\n",
            "Batch done. Current results length: 32512\n",
            "Batch done. Current results length: 32640\n",
            "Batch done. Current results length: 32768\n",
            "Batch done. Current results length: 32896\n",
            "Batch done. Current results length: 33024\n",
            "Batch done. Current results length: 33152\n",
            "Batch done. Current results length: 33280\n",
            "Batch done. Current results length: 33408\n",
            "Batch done. Current results length: 33536\n",
            "Batch done. Current results length: 33664\n",
            "Batch done. Current results length: 33792\n",
            "Batch done. Current results length: 33920\n",
            "Batch done. Current results length: 34048\n",
            "Batch done. Current results length: 34176\n",
            "Batch done. Current results length: 34304\n",
            "Batch done. Current results length: 34432\n",
            "Batch done. Current results length: 34560\n",
            "Batch done. Current results length: 34688\n",
            "Batch done. Current results length: 34816\n",
            "Batch done. Current results length: 34944\n",
            "Batch done. Current results length: 35072\n",
            "Batch done. Current results length: 35200\n",
            "Batch done. Current results length: 35328\n",
            "Batch done. Current results length: 35456\n",
            "Batch done. Current results length: 35584\n",
            "Batch done. Current results length: 35712\n",
            "Batch done. Current results length: 35840\n",
            "Batch done. Current results length: 35968\n",
            "Batch done. Current results length: 36096\n",
            "Batch done. Current results length: 36224\n",
            "Batch done. Current results length: 36352\n",
            "Batch done. Current results length: 36480\n",
            "Batch done. Current results length: 36608\n",
            "Batch done. Current results length: 36736\n",
            "Batch done. Current results length: 36864\n",
            "Batch done. Current results length: 36992\n",
            "Batch done. Current results length: 37120\n",
            "Batch done. Current results length: 37248\n",
            "Batch done. Current results length: 37376\n",
            "Batch done. Current results length: 37504\n",
            "Batch done. Current results length: 37632\n",
            "Batch done. Current results length: 37760\n",
            "Batch done. Current results length: 37888\n",
            "Batch done. Current results length: 38016\n",
            "Batch done. Current results length: 38144\n",
            "Batch done. Current results length: 38272\n",
            "Batch done. Current results length: 38400\n",
            "Batch done. Current results length: 38528\n",
            "Batch done. Current results length: 38656\n",
            "Batch done. Current results length: 38784\n",
            "Batch done. Current results length: 38912\n",
            "Batch done. Current results length: 39040\n",
            "Batch done. Current results length: 39168\n",
            "Batch done. Current results length: 39296\n",
            "Batch done. Current results length: 39424\n",
            "Batch done. Current results length: 39552\n",
            "Batch done. Current results length: 39680\n",
            "Batch done. Current results length: 39808\n",
            "Batch done. Current results length: 39936\n",
            "Batch done. Current results length: 40064\n",
            "Batch done. Current results length: 40192\n",
            "Batch done. Current results length: 40320\n",
            "Batch done. Current results length: 40448\n",
            "Batch done. Current results length: 40576\n",
            "Batch done. Current results length: 40704\n",
            "Batch done. Current results length: 40832\n",
            "Batch done. Current results length: 40960\n",
            "Batch done. Current results length: 41088\n",
            "Batch done. Current results length: 41216\n",
            "Batch done. Current results length: 41344\n",
            "Batch done. Current results length: 41472\n",
            "Batch done. Current results length: 41600\n",
            "Batch done. Current results length: 41728\n",
            "Batch done. Current results length: 41856\n",
            "Batch done. Current results length: 41984\n",
            "Batch done. Current results length: 42112\n",
            "Batch done. Current results length: 42240\n",
            "Batch done. Current results length: 42368\n",
            "Batch done. Current results length: 42496\n",
            "Batch done. Current results length: 42624\n",
            "Batch done. Current results length: 42752\n",
            "Batch done. Current results length: 42880\n",
            "Batch done. Current results length: 43008\n",
            "Batch done. Current results length: 43136\n",
            "Batch done. Current results length: 43264\n",
            "Batch done. Current results length: 43392\n",
            "Batch done. Current results length: 43520\n",
            "Batch done. Current results length: 43648\n",
            "Batch done. Current results length: 43776\n",
            "Batch done. Current results length: 43904\n",
            "Batch done. Current results length: 44032\n",
            "Batch done. Current results length: 44160\n",
            "Batch done. Current results length: 44288\n",
            "Batch done. Current results length: 44416\n",
            "Batch done. Current results length: 44544\n",
            "Batch done. Current results length: 44672\n",
            "Batch done. Current results length: 44800\n",
            "Batch done. Current results length: 44928\n",
            "Batch done. Current results length: 45056\n",
            "Batch done. Current results length: 45184\n",
            "Batch done. Current results length: 45312\n",
            "Batch done. Current results length: 45440\n",
            "Batch done. Current results length: 45568\n",
            "Batch done. Current results length: 45696\n",
            "Batch done. Current results length: 45824\n",
            "Batch done. Current results length: 45952\n",
            "Batch done. Current results length: 46080\n",
            "Batch done. Current results length: 46208\n",
            "Batch done. Current results length: 46336\n",
            "Batch done. Current results length: 46464\n",
            "Batch done. Current results length: 46592\n",
            "Batch done. Current results length: 46720\n",
            "Batch done. Current results length: 46848\n",
            "Batch done. Current results length: 46976\n",
            "Batch done. Current results length: 47104\n",
            "Batch done. Current results length: 47232\n",
            "Batch done. Current results length: 47360\n",
            "Batch done. Current results length: 47488\n",
            "Batch done. Current results length: 47616\n",
            "Batch done. Current results length: 47744\n",
            "Batch done. Current results length: 47872\n",
            "Batch done. Current results length: 48000\n",
            "Batch done. Current results length: 48128\n",
            "Batch done. Current results length: 48256\n",
            "Batch done. Current results length: 48384\n",
            "Batch done. Current results length: 48512\n",
            "Batch done. Current results length: 48640\n",
            "Batch done. Current results length: 48768\n",
            "Batch done. Current results length: 48896\n",
            "Batch done. Current results length: 49024\n",
            "Batch done. Current results length: 49152\n",
            "Batch done. Current results length: 49280\n",
            "Batch done. Current results length: 49408\n",
            "Batch done. Current results length: 49536\n",
            "Batch done. Current results length: 49664\n",
            "Batch done. Current results length: 49792\n",
            "Batch done. Current results length: 49920\n",
            "Batch done. Current results length: 50000\n",
            "Final results length: 50000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#_,subset_3_loader=get_subset_loader_1000(full_dataset,5000)\\ntraining_data_3=get_training_data(201,300,data_loader)\\n\\n#_,subset_4_loader=get_subset_loader_1000(full_dataset,5000)\\ntraining_data_4=get_training_data(301,400,data_loader)\\n\\n#_,subset_5_loader=get_subset_loader_1000(full_dataset,5000)\\ntraining_data_5=get_training_data(401,500,data_loader)\\n\\n#_,subset_6_loader=get_subset_loader_1000(full_dataset,5000)\\ntraining_data_6=get_training_data(501,600,data_loader)\\n\\n#_,subset_7_loader=get_subset_loader_1000(full_dataset,5000)\\ntraining_data_7=get_training_data(601,700,data_loader)\\n\\n#_,subset_8_loader=get_subset_loader_1000(full_dataset,5000)\\ntraining_data_8=get_training_data(701,800,data_loader)\\n\\n#_,subset_9_loader=get_subset_loader_1000(full_dataset,5000)\\ntraining_data_9=get_training_data(801,900,data_loader)\\n\\n#_,subset_10_loader=get_subset_loader_1000(full_dataset,5000)\\ntraining_data_10=get_training_data(901,999,data_loader)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "#get the training data for 10 different classifiers\n",
        "folder_path = '/content/drive/My Drive/classifier_noised_training_data'\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "#_,subset_1_loader=get_subset_loader_1000(full_dataset,5000)\n",
        "#training_data_1=get_training_data(1,100,data_loader)\n",
        "#training_data_1.to_csv(f'{folder_path}/training_data_1.csv', index=False)\n",
        "\n",
        "#_,subset_2_loader=get_subset_loader_1000(full_dataset,5000)\n",
        "#training_data_2=get_training_data(101,200,data_loader)\n",
        "#training_data_2.to_csv(f'{folder_path}/training_data_1.csv', index=False)\n",
        "\n",
        "#_,subset_3_loader=get_subset_loader_1000(full_dataset,5000)\n",
        "training_data_3=get_training_data(201,300,data_loader)\n",
        "'''\n",
        "#_,subset_4_loader=get_subset_loader_1000(full_dataset,5000)\n",
        "training_data_4=get_training_data(301,400,data_loader)\n",
        "\n",
        "#_,subset_5_loader=get_subset_loader_1000(full_dataset,5000)\n",
        "training_data_5=get_training_data(401,500,data_loader)\n",
        "\n",
        "#_,subset_6_loader=get_subset_loader_1000(full_dataset,5000)\n",
        "training_data_6=get_training_data(501,600,data_loader)\n",
        "\n",
        "#_,subset_7_loader=get_subset_loader_1000(full_dataset,5000)\n",
        "training_data_7=get_training_data(601,700,data_loader)\n",
        "\n",
        "#_,subset_8_loader=get_subset_loader_1000(full_dataset,5000)\n",
        "training_data_8=get_training_data(701,800,data_loader)\n",
        "\n",
        "#_,subset_9_loader=get_subset_loader_1000(full_dataset,5000)\n",
        "training_data_9=get_training_data(801,900,data_loader)\n",
        "\n",
        "#_,subset_10_loader=get_subset_loader_1000(full_dataset,5000)\n",
        "training_data_10=get_training_data(901,999,data_loader)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veVlrRd7xyyX"
      },
      "outputs": [],
      "source": [
        "#define the custom dataset class\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomTensorDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 假设第一列是转换后的图像，第二列是标签\n",
        "        image = self.dataframe.iloc[idx, 0]  # 这里得到的是一个图像张量\n",
        "        label = self.dataframe.iloc[idx, 1]\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsfQjO6Gx9Hf"
      },
      "outputs": [],
      "source": [
        "#split the train test dataset for 10 subsets\n",
        "#train_df_1, test_df_1 = train_test_split(training_data_1, test_size=0.2, random_state=42)\n",
        "train_df_2, test_df_2 = train_test_split(training_data_2, test_size=0.2, random_state=42)\n",
        "'''train_df_3, test_df_3 = train_test_split(training_data_3, test_size=0.2, random_state=42)\n",
        "train_df_4, test_df_4 = train_test_split(training_data_4, test_size=0.2, random_state=42)\n",
        "train_df_5, test_df_5 = train_test_split(training_data_5, test_size=0.2, random_state=42)\n",
        "train_df_6, test_df_6 = train_test_split(training_data_6, test_size=0.2, random_state=42)\n",
        "train_df_7, test_df_7 = train_test_split(training_data_7, test_size=0.2, random_state=42)\n",
        "train_df_8, test_df_8 = train_test_split(training_data_8, test_size=0.2, random_state=42)\n",
        "train_df_9, test_df_9 = train_test_split(training_data_9, test_size=0.2, random_state=42)\n",
        "train_df_10, test_df_10 = train_test_split(training_data_10, test_size=0.2, random_state=42)'''\n",
        "\n",
        "#create train and test datasets\n",
        "#train_dataset_1 = CustomTensorDataset(train_df_1)\n",
        "#test_dataset_1 = CustomTensorDataset(test_df_1)\n",
        "\n",
        "#train_dataset_2 = CustomTensorDataset(train_df_2)\n",
        "test_dataset_2 = CustomTensorDataset(test_df_2)\n",
        "'''\n",
        "train_dataset_3 = CustomTensorDataset(train_df_3)\n",
        "test_dataset_3 = CustomTensorDataset(test_df_3)\n",
        "\n",
        "train_dataset_4 = CustomTensorDataset(train_df_4)\n",
        "test_dataset_4 = CustomTensorDataset(test_df_4)\n",
        "\n",
        "train_dataset_5 = CustomTensorDataset(train_df_5)\n",
        "test_dataset_5 = CustomTensorDataset(test_df_5)\n",
        "\n",
        "train_dataset_6 = CustomTensorDataset(train_df_6)\n",
        "test_dataset_6 = CustomTensorDataset(test_df_6)\n",
        "\n",
        "train_dataset_7 = CustomTensorDataset(train_df_7)\n",
        "test_dataset_7 = CustomTensorDataset(test_df_7)\n",
        "\n",
        "train_dataset_8 = CustomTensorDataset(train_df_8)\n",
        "test_dataset_8 = CustomTensorDataset(test_df_8)\n",
        "\n",
        "train_dataset_9 = CustomTensorDataset(train_df_9)\n",
        "test_dataset_9 = CustomTensorDataset(test_df_9)\n",
        "\n",
        "train_dataset_10 = CustomTensorDataset(train_df_10)\n",
        "test_dataset_10 = CustomTensorDataset(test_df_10)'''\n",
        "\n",
        "#create train and test dataloaders\n",
        "#train_loader_1 = DataLoader(train_dataset_1, batch_size=32, shuffle=True)\n",
        "#test_loader_1 = DataLoader(test_dataset_1, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_2 = DataLoader(train_dataset_2, batch_size=32, shuffle=True)\n",
        "test_loader_2 = DataLoader(test_dataset_2, batch_size=32, shuffle=False)\n",
        "'''\n",
        "train_loader_3 = DataLoader(train_dataset_3, batch_size=32, shuffle=True)\n",
        "test_loader_3 = DataLoader(test_dataset_3, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_4 = DataLoader(train_dataset_4, batch_size=32, shuffle=True)\n",
        "test_loader_4 = DataLoader(test_dataset_4, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_5 = DataLoader(train_dataset_5, batch_size=32, shuffle=True)\n",
        "test_loader_5 = DataLoader(test_dataset_5, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_6 = DataLoader(train_dataset_6, batch_size=32, shuffle=True)\n",
        "test_loader_6 = DataLoader(test_dataset_6, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_7 = DataLoader(train_dataset_7, batch_size=32, shuffle=True)\n",
        "test_loader_7 = DataLoader(test_dataset_7, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_8 = DataLoader(train_dataset_8, batch_size=32, shuffle=True)\n",
        "test_loader_8 = DataLoader(test_dataset_8, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_9 = DataLoader(train_dataset_9, batch_size=32, shuffle=True)\n",
        "test_loader_9 = DataLoader(test_dataset_9, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_10 = DataLoader(train_dataset_10, batch_size=32, shuffle=True)\n",
        "test_loader_10 = DataLoader(test_dataset_10, batch_size=32, shuffle=False)'''\n",
        "\n",
        "train_loader_lst=[train_loader_2]\n",
        "test_loader_lst=[test_loader_2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j704OPv8vcp6"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "#create 10 different classifier models\n",
        "classifier1=models.resnet50(pretrained=True)\n",
        "classifier1.fc=nn.Sequential(\n",
        "    nn.Linear(in_features=2048,out_features=1000,bias=True),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Linear(in_features=1000,out_features=10,bias=True),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n",
        "classifier2=models.resnet50(pretrained=True)\n",
        "classifier2.fc=nn.Sequential(\n",
        "    nn.Linear(in_features=2048,out_features=1000,bias=True),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Linear(in_features=1000,out_features=10,bias=True),\n",
        "    #nn.LogSoftmax(dim=1)\n",
        ")\n",
        "'''classifier3=models.resnet50(pretrained=True)\n",
        "classifier4=models.resnet50(pretrained=True)\n",
        "classifier5=models.resnet50(pretrained=True)\n",
        "classifier6=models.resnet50(pretrained=True)\n",
        "classifier7=models.resnet50(pretrained=True)\n",
        "classifier8=models.resnet50(pretrained=True)\n",
        "classifier9=models.resnet50(pretrained=True)\n",
        "classifier10=models.resnet50(pretrained=True)'''\n",
        "\n",
        "classifier_list=[classifier1]\n",
        "for classifier in classifier_list:\n",
        "  classifier.to('cuda')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_list=[]\n",
        "for classifier in classifier_list:\n",
        "    optimizer=optim.Adam(classifier.parameters(), lr=0.001)\n",
        "    optimizer_list.append(optimizer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCkKe9_KT9Wh"
      },
      "outputs": [],
      "source": [
        "classifier_clean=models.resnet50(pretrained=True)\n",
        "optimizer_clean=optim.Adam(classifier_clean.parameters(),lr=0.001)\n",
        "optimizer_list_clean=[optimizer_clean]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkfzGP6JdosA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d257fde8-49ff-4595-e2fa-d3f4eda2e28f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(classifier_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1ugwwEcd8E8"
      },
      "outputs": [],
      "source": [
        "#add softmax to the classifier and set the output dimension to be equal to the number of classes\n",
        "import torch.nn as nn\n",
        "classifier_clean.fc=nn.Sequential(\n",
        "    nn.Linear(in_features=2048,out_features=1000,bias=True),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Linear(in_features=1000,out_features=10,bias=True),\n",
        "    #nn.LogSoftmax(dim=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozsReK_yeMHA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beccbc76-4744-4a2c-a649-f1fc54412344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=2048, out_features=1000, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=1000, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(classifier_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbhV2MKpeBSh"
      },
      "outputs": [],
      "source": [
        "classifier_clean=classifier_clean.to('cuda')\n",
        "classifier_clean_list=[classifier_clean]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZVkdvgI04bE"
      },
      "outputs": [],
      "source": [
        "#define the training function\n",
        "def evaluate_accuracy(model, test_loader):\n",
        "    # 确保模型在验证模式，这对于某些层如Dropout和BatchNorm是必要的\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # 在评估模式下不计算梯度\n",
        "        for data in test_loader:\n",
        "            label_tensor=data[1]\n",
        "            input_tensor=data[0]\n",
        "            label_tensor=label_tensor.to('cuda')\n",
        "            input_tensor=input_tensor.to('cuda')\n",
        "            outputs = model(input_tensor)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += label_tensor.size(0)\n",
        "            correct += (predicted == label_tensor).sum().item()\n",
        "\n",
        "    # 转换为百分比\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "def train(model_list, train_loader_list, test_loader_list, criterion, optimizer_list, num_epochs=10):\n",
        "    count_model=0\n",
        "\n",
        "    new_lst=[]\n",
        "\n",
        "    for i in range(len(model_list)):\n",
        "      model=model_list[i]\n",
        "      model.train()\n",
        "\n",
        "      train_loader=train_loader_list[i]\n",
        "      test_loader=test_loader_list[i]\n",
        "      optimizer=optimizer_list[i]\n",
        "\n",
        "      for epoch in range(num_epochs):\n",
        "          running_loss = 0.0\n",
        "          correct = 0\n",
        "          total = 0\n",
        "\n",
        "          # 训练过程\n",
        "          for data in train_loader:\n",
        "              #labels, inputs = data[0], data[1]\n",
        "              label_tensor=data[1]\n",
        "              input_tensor=data[0]\n",
        "              #print(label_tensor)\n",
        "              #print(input_tensor)\n",
        "              label_tensor=label_tensor.to('cuda')\n",
        "              input_tensor=input_tensor.to('cuda')\n",
        "              #print(label_tensor)\n",
        "              optimizer.zero_grad()  # 清空之前的梯度\n",
        "              #print(input_tensor.size())\n",
        "              outputs = model(input_tensor)  # 获得模型输出\n",
        "              #print(outputs.size())\n",
        "              #print(outputs.shape)\n",
        "              #print(label_tensor.shape)\n",
        "              loss = criterion(outputs, label_tensor)  # 计算损失\n",
        "              loss.backward()  # 反向传播\n",
        "              optimizer.step()  # 更新权重\n",
        "\n",
        "              running_loss += loss.item()\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              #print('this is predicted',predicted)\n",
        "              total += label_tensor.size(0)\n",
        "              correct += (predicted == label_tensor).sum().item()\n",
        "\n",
        "          # 训练完一个epoch后的平均损失\n",
        "          epoch_loss = running_loss / len(train_loader)\n",
        "\n",
        "          # 计算测试集上的准确率\n",
        "          test_accuracy = evaluate_accuracy(model, test_loader)\n",
        "\n",
        "          print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "          model_path = model_path = '/content/drive/MyDrive/CV/epoch_{}_fine_tuned_resnet50.pth'.format(epoch)\n",
        "          torch.save(model.state_dict(), model_path)\n",
        "      count_model+=1\n",
        "\n",
        "      print('Finished Training',count_model)\n",
        "\n",
        "      new_lst.append(model)\n",
        "\n",
        "    return new_lst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9Bw2Wof19cZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c41aac-7413-4567-817a-5bf6d6d548f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.3775, Test Accuracy: 64.29%\n",
            "Epoch 2/10, Loss: 1.9195, Test Accuracy: 40.26%\n",
            "Epoch 3/10, Loss: 1.4930, Test Accuracy: 49.49%\n",
            "Epoch 4/10, Loss: 1.2868, Test Accuracy: 57.05%\n",
            "Epoch 5/10, Loss: 1.1120, Test Accuracy: 59.72%\n",
            "Epoch 6/10, Loss: 0.9604, Test Accuracy: 61.91%\n",
            "Epoch 7/10, Loss: 0.8273, Test Accuracy: 66.34%\n",
            "Epoch 8/10, Loss: 0.7031, Test Accuracy: 67.47%\n",
            "Epoch 9/10, Loss: 0.5826, Test Accuracy: 68.16%\n",
            "Epoch 10/10, Loss: 0.4657, Test Accuracy: 68.56%\n",
            "Finished Training 1\n"
          ]
        }
      ],
      "source": [
        "new_model_list=trained_model_list=train(classifier_list, train_loader_lst, test_loader_lst, criterion, optimizer_list, num_epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7ltzyt9UZP8",
        "outputId": "a1c6cbf2-98cb-4fbd-f1d7-c819782e2505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.8492, Test Accuracy: 77.97%\n",
            "Epoch 2/10, Loss: 0.5834, Test Accuracy: 78.65%\n",
            "Epoch 3/10, Loss: 0.4267, Test Accuracy: 80.97%\n",
            "Epoch 4/10, Loss: 0.3166, Test Accuracy: 81.04%\n",
            "Epoch 5/10, Loss: 0.2273, Test Accuracy: 79.94%\n",
            "Epoch 6/10, Loss: 0.1756, Test Accuracy: 81.06%\n",
            "Epoch 7/10, Loss: 0.1422, Test Accuracy: 80.01%\n",
            "Epoch 8/10, Loss: 0.1349, Test Accuracy: 80.30%\n",
            "Epoch 9/10, Loss: 0.1188, Test Accuracy: 79.75%\n",
            "Epoch 10/10, Loss: 0.1020, Test Accuracy: 80.21%\n",
            "Finished Training 1\n"
          ]
        }
      ],
      "source": [
        "new_clean_model_list=trained_model_list=train(classifier_list, train_loader_full_list, test_loader_full_list, criterion, optimizer_list, num_epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUrr-bee6OPG",
        "outputId": "f62b66ab-773a-4b31-d1c7-336c93890fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.55\n"
          ]
        }
      ],
      "source": [
        "#for the result we get bby training on the noisy data is too low, we directly use the model trained on the clean data to\n",
        "#predict on the test data which was added noise, and check the accuracy\n",
        "accuracy=evaluate_accuracy(classifier_clean, test_loader_2)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZxWpUBTs1Fw"
      },
      "outputs": [],
      "source": [
        "# #save the state dictionary of thfe fintuned resnet 50 model on the cifar 10 dataset\n",
        "# model_path = '/content/drive/MyDrive/Classifier_Folder/fine_tuned_resnet50_full.pth'\n",
        "# torch.save(classifier1.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BttQ87z3AWb"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/CV/fine_tuned_resnet50_noisy_100-200.pth'\n",
        "torch.save(classifier1.state_dict(), model_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}